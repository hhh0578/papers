# [Neural Discrete Representation Learning](https://arxiv.org/abs/1711.00937)
[参考代码](https://github.com/1Konny/VQ-VAE)
## 摘要
无师学习获取有效表达在机器学习领域是一项重要挑战。本文给出了简单有效的生成模型，用以生成离散表达。本模型Vector Quantised-Variatioanl AutoEncoder（VQ-VAE）比起VAE，在两个方面有所区别：
1. encoder网络输出离散，而非连续。
2. 先验条件通过学习得到，而非静态。

用本模型可以回避“后验坍塌”——使用强大的自动回归式decoder时会忽略潜在要素。这是VAE框架中的常见问题。将这些表达与自动回归式先验配对对齐，可以让模型生成高质量图像、视频、演讲、或是对话。而且可以无师学习发音，让学到的表达更有用。
## 简介
图像、音频、视频的采样和应用方面，生成模型已经十分先进。但同时，在few-shot learning、domain adaptation、reinforcement learning仍旧依赖于从源数据中提取的表达。但是现有的无师学习依旧难以说是有效。  
最大似然和重构误差是无师学习的常用方法，然而其有效性取决于实际采用的属性。我们的目标是建立一种模型，能够在潜在空间中保留数据中的重要属性，同时达到最大似然度。  

现有研究已经给出了许多在连续空间中得到表达的方法，然而我们注目于有较大研究空间的离散表达，这更贴近我们研究的主旨。语言是离散的，类似的演讲也是一列符号，图片可以通过语言简要表达。进一步说，离散表达更适合学习复杂的推理、计划和预测。在深度学习中使用离散潜在变量的困难程度已经证实，而一些建立在离散变量上的强大自动回归式模型也被提出。  
我们给出一种新型生成模型，这种模型通过参数化观察结果给出的后验（离散）分布变量，成功将变分自动编码器和离散潜在表达相结合。我们的模型利用vector quantization（VQ），易于学习，避免了大方差和“后验坍塌”。另外，这是第一个达到了等同于连续模型效果的离散潜在VAE模型，且保留了离散分布的灵活性。我们称其为VQ-VAE  
VQ-VAE模型活用了潜在空间，从源数据的大空间中提取出重要属性，剔除了那些难以察觉的噪音占据的大量空间。  

最后，在VQ-VAE给出的优良潜在结构下，我们在离散随机变量上训练出有效的先验，得到了一些有趣的例子和应用。比如，在演讲上无师学习得到了发音和单词。进一步说，我们还能在decoder上配上演讲人属性，这就能够生成对话，从一个人切换到另一个人的时候无需变动内容。我们同样给出了强化学习中长期环境下的可靠结果。  
文章构成总结如下：
1. 介绍VQ-VAE模型。该模型利用了离散潜在要素，学习简单，没有“后验坍塌”和方差问题。
2. 我们会展现这种离散潜在模型在log似然度上能和连续模型匹敌。
3. 在强大的先验条件下，我们的模型能在各个方面得出相干的高质量结果。如演讲和视频生成。
4. 我们将从演讲中学习语言，无需任何监督，并且应用到无师对话上。
## 相关研究
现存的离散VAE：
- NVIL，NVIL估计器使用单样本目标来优化变分下界，并使用各种减少方差的技术来加快训练速度。
- VIMCO，优化多样本目标，并利用生成网络得到的多样本加快收敛。
- Concrete和Gumbel-softmax分布。

然而由于能减低梯度方差的Gaussian reparameterization技巧，上述方法都没能缩小与连续潜在VAE之间的差距。而且，上述实验都只在小数据集上检验，潜在分布的维度很小。本实验用了复杂的图片数据集（CIFAR10、ImageNet和DeepMind Lab）和演讲数据集（VCTK）  

最后，我们的方法还和神经网络图片压缩技术有关。Theis利用算数编码之前的标量量化步骤压缩有损图片。其他作者给出了一种利用vector quantisation的类似手法。作者提出矢量量化的连续松弛，将其随时间退火以获得硬聚类。他们在实验中先训练自编码器，然后将矢量量化用在编码器的激活函数上，最后使用从软到硬的松弛以及很小的学习率对整个网络进行微调。在我们的实验中，由于解码器始终能够在训练过程中反转连续松弛，因此我们无法从头开始使用软到硬松弛方法进行训练，因此没有进行实际的量化。
## VQ-VAE
与我们的研究关系最为密切的是VAE。VAEyou两部分组成：encoder网络在给定的数据*x*，即先验分布*p（z）*下，参数化离散潜在随机变量*z*的后验分布*q（z|x）*；然后decoder网络给出概率分布*p（x|z）*。

一般而言，VAE中的后验和先验认为是协方差分布，这就能用Gaussian reparameterization技巧。以此为基础的模型有自回归先验模型和后验模型，normalising flows和逆自回归后验模型。

本文我们通过离散潜在变量的新学习法来介绍VQ-VAE模型。后验概率和先验概率是多项式分布，而这些分布中得到的样本会索引一张嵌入表。这张嵌入表用于decoder网络的输入。
### 离散潜变量
我们定义潜嵌入空间e&isin;R<sup>KxD</sup>，其中K是离散潜空间的大小，D是每个潜嵌入向量的维度e<sub>i</sub>。按图1，模型输入x，得到z<sub>e</sub>（x）。然后按照公式1，从共有的嵌入空间e中计算出最接近的离散潜变量z。decoder的输入对应公式2中的嵌入向量e<sub>k</sub>。这种前向计算可以视作将潜变量非线性映射到1-of-K 嵌入向量的常规自编码。这个模型的参数集是encoder、decoder和嵌入空间e的合集。为求方便，本节中我们用随机变量z来代表离散潜变量。在实际的演讲、图片、视频中我们分别提取1D、2D、3D潜特征空间。

后验多项式分布q（z|x）按下述one-hot公式定义：  
![Imgur](https://i.imgur.com/ZMddlYp.png)

其中z（x）代表encoder网络的输出。我们将此模型视作VAE，可以将log p（x）和ELBO绑定。proposal distribution q（z = k | x）是确定性的，并且通过在z上定义一个简单的uniform先验，我们可以获得KL散度常数，等于log K。

表达z（x）穿过discretisation bottleneck，按照公式1和2在最近的嵌入元素上映射。
![Imgur](https://i.imgur.com/RwY7Lyu.png)

![Imgur](https://i.imgur.com/2x1WSCH.png)
### 学习
注意公式2中没有实际梯度，但我们认为这里梯度和直接估计相似，将decoder输入zq（x）的梯度复制到encoder的输出ze（x）。当然可以通过量化计算subgradient，但是本文的实验直接复制依旧效果良好。

在前向计算中，nearest embedding zq（x）传入decoder，然后再逆向传播中将梯度&nabla;zL直接送回encoder。既然encoder的输出和decoder的输入共享同样维度D的空间，这个梯度就保存了能调整encoder以降低重构损失的有用信息。

看图1（右边），由于公式1会变化，梯度会让下一次前向传播的encoder输出发生变化。

公式3定义了总损失函数。它由三个部分组成，分别训练VQ-VAE的不同部分。  
第一个项是重构误差（或者说是数据项），优化encoder和decoder。由于梯度直接从ze（x）传到zq（x），嵌入ei不会从重构损失中得到梯度。因此，  
第二项，为了训练嵌入空间，我们使用最简单的字典学习算法之一，Vector Quantisation。VQ目标利用L2误差调整嵌入向量ei趋近encoder输出ze（x）。由于这一项仅仅作用于更新字典，因此还可以根据ze（x）的移动平均值来更新字典项。详情见附录A1。

最后，由于嵌入空间的体积是无量纲的，因此如果嵌入ei的训练速度不如encoder参数那么快，则可以任意增长。为了确保编码器进行嵌入并且其输出不会增长，我们添加了一个承诺损失，即等式3中的第三项。
由此得到训练目标：  
![Imgur](https://i.imgur.com/Hg5ya1t.png)

其中sg代表stopgradient operator，这代表在前向计算的时候偏微分为0，即是个不更新的常量。decoder由第一项优化，encoder由第一项和第三项优化，embeddings从第二项进行优化。我们发现最终算法在&beta;上十分顽抗，在&beta;=0.1到2.0上并没有多大变化。在所有的实验中&beta;=0.25。由于我们假设z是uniform先验，因此通常在ELBO中出现的KL项相对于编码器参数是恒定的，因此在训练中可以忽略。

我们的实验中定义N离散latents（在ImageNet是32x32，在CIFAR10是8x8x10）。损失L是相同的，只是会针对每个latent得到N terms上k-means和commitment loss的一个平均值。

![Imgur](https://i.imgur.com/aqqleQu.png)

