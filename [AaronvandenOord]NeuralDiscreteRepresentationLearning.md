# [Neural Discrete Representation Learning](https://arxiv.org/abs/1711.00937)
[参考代码](https://github.com/1Konny/VQ-VAE)
## 摘要
无师学习获取有效表达在机器学习领域是一项重要挑战。本文给出了简单有效的生成模型，用以生成离散表达。本模型Vector Quantised-Variatioanl AutoEncoder（VQ-VAE）比起VAE，在两个方面有所区别：
1. encoder网络输出离散，而非连续。
2. 先验条件通过学习得到，而非静态。

用本模型可以回避“后验坍塌”——使用强大的自动回归式decoder时会忽略潜在要素。这是VAE框架中的常见问题。将这些表达与自动回归式先验配对对齐，可以让模型生成高质量图像、视频、演讲、或是对话。而且可以无师学习发音，让学到的表达更有用。
## 简介
图像、音频、视频的采样和应用方面，生成模型已经十分先进。但同时，在few-shot learning、domain adaptation、reinforcement learning仍旧依赖于从源数据中提取的表达。但是现有的无师学习依旧难以说是有效。  
最大似然和重构误差是无师学习的常用方法，然而其有效性取决于实际采用的属性。我们的目标是建立一种模型，能够在潜在空间中保留数据中的重要属性，同时达到最大似然度。  

现有研究已经给出了许多在连续空间中得到表达的方法，然而我们注目于有较大研究空间的离散表达，这更贴近我们研究的主旨。语言是离散的，类似的演讲也是一列符号，图片可以通过语言简要表达。进一步说，离散表达更适合学习复杂的推理、计划和预测。在深度学习中使用离散潜在变量的困难程度已经证实，而一些建立在离散变量上的强大自动回归式模型也被提出。  
我们给出一种新型生成模型，这种模型通过参数化观察结果给出的后验（离散）分布变量，成功将变分自动编码器和离散潜在表达相结合。我们的模型利用vector quantization（VQ），易于学习，避免了大方差和“后验坍塌”。另外，这是第一个达到了等同于连续模型效果的离散潜在VAE模型，且保留了离散分布的灵活性。我们称其为VQ-VAE  
VQ-VAE模型活用了潜在空间，从源数据的大空间中提取出重要属性，剔除了那些难以察觉的噪音占据的大量空间。  

最后，在VQ-VAE给出的优良潜在结构下，我们在离散随机变量上训练出有效的先验，得到了一些有趣的例子和应用。比如，在演讲上无师学习得到了发音和单词。进一步说，我们还能在decoder上配上演讲人属性，这就能够生成对话，从一个人切换到另一个人的时候无需变动内容。我们同样给出了强化学习中长期环境下的可靠结果。  
文章构成总结如下：
1. 介绍VQ-VAE模型。该模型利用了离散潜在要素，学习简单，没有“后验坍塌”和方差问题。
2. 我们会展现这种离散潜在模型在log似然度上能和连续模型匹敌。
3. 在强大的先验条件下，我们的模型能在各个方面得出相干的高质量结果。如演讲和视频生成。
4. 我们将从演讲中学习语言，无需任何监督，并且应用到无师对话上。
## 相关研究
现存的离散VAE：
- NVIL，NVIL估计器使用单样本目标来优化变分下界，并使用各种减少方差的技术来加快训练速度。
- VIMCO，优化多样本目标，并利用生成网络得到的多样本加快收敛。
- Concrete和Gumbel-softmax分布。

然而由于能减低梯度方差的Gaussian reparameterisation技巧，上述方法都没能缩小与连续潜在VAE之间的差距。而且，上述实验都只在小数据集上检验，潜在分布的维度很小。本实验用了复杂的图片数据集（CIFAR10、ImageNet和DeepMind Lab）和演讲数据集（VCTK）  

最后，我们的方法还和神经网络图片压缩技术有关。Theis利用算数编码之前的标量量化步骤压缩有损图片。其他作者给出了一种利用vector quantisation的类似手法。作者提出矢量量化的连续松弛，将其随时间退火以获得硬聚类。他们在实验中先训练自编码器，然后将矢量量化用在编码器的激活函数上，最后使用从软到硬的松弛以及很小的学习率对整个网络进行微调。在我们的实验中，由于解码器始终能够在训练过程中反转连续松弛，因此我们无法从头开始使用软到硬松弛方法进行训练，因此没有进行实际的量化。
## VQ-VAE
