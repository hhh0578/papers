# [MONOTONIC MULTIHEAD ATTENTION](https://arxiv.org/abs/1909.12406)
[参考代码](https://github.com/pytorch/fairseq/blob/master/examples/simultaneous_translation/models/transformer_monotonic_attention.py)
